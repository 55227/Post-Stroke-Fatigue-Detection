import torch
import torch.nn as nn
import numpy as np

from network.CNN_RNN.cnn_model import VisualModel
from network.rnn.rnn import RNN


class VisualRNNModel(nn.Module):

    def __init__(self,
                 visualModelName: str,
                 pretrained: bool = False,
                 num_classes: int = 4):
        """ Visual model with an RNN on top. The obtained model structure: a model named after `model_name` from
            torchvision (discard the last fully connected layer)->two-layer gated recurrent unit RNN->a fully connected layer

        Args:
            model_name (str): Which visual model to use.
            pretrained (bool): Use pretrained (on ImageNet) model (default `False`).
            num_outs (int): number of output values of the model (default `2`).
        """

        super(VisualRNNModel, self).__init__()

        self.visual_model = VisualModel(visualModelName, pretrained)
        num_out_features = self.visual_model.num_features# the number of ouput features generated by `self.visual_model`

        self.rnn, num_out_features = self._get_rnn_model(num_out_features)
        self.linear = nn.Linear(num_out_features, num_classes)
        self.num_outs = num_classes

    def _get_rnn_model(self, input_size: int):
        """ Builder method to instantiate an RNN object. Builds a 2-layer gated recurrent unit RNN"""

        rnn_args = {
            'input_size': input_size,
            'hidden_size': 256,# the half of length of output features in resnet18 or resnet34
            'num_layers': 2,
            'batch_first': True # the input and output tensors are provided as (batch,seq,feature)
        }
        return RNN(rnn_args, 'lstm'), rnn_args['hidden_size']

    def forward(self, x):
        """ Forward pass.

        Args:
            x (batch_size,seq_length,channels,height,width)
        """


        batch_size,  seq_length, c,h, w = x.shape

        x = x.view(batch_size * seq_length, c, h, w)# (batch_size*seq_length,channels,height,width)

        visual_out = self.visual_model(x)# （batch_size*seq_length,out_channels,*）

        visual_out = visual_out.contiguous().view(batch_size, seq_length, -1)

        rnn_out, _ = self.rnn(visual_out)

        output = self.linear(rnn_out)

        return output

    def get_1x_lr_params(self):
        """
            This generator returns all the parameters for the visual model.
            """
        # print('----')
        for name, param in self.visual_model.named_parameters():
            if param.requires_grad:
                # print(name)
                yield param

    def get_10x_lr_params(self):
        """
           This generator returns all the parameters for the RNN and the last linear layers of the model.
           """
        # print('===========')
        for name, param in self.named_parameters():
            if 'visual_model' not in name and param.requires_grad:
                # print(param)
                yield param





if __name__=="__main__":
    model=VisualRNNModel('resnet101',pretrained=True)
    state=model.state_dict()
    model.get_1x_lr_params()
    model.get_10x_lr_params()
    # train_params = [{'params': model.get_1x_lr_params(), 'lr': 1},
    #                 {'params': model.get_10x_lr_params(), 'lr': 1 * 10}]
    # print(train_params)